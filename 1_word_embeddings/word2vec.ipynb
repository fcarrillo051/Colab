{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fcarrillo051/SomosNLP/blob/main/1_word_embeddings/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvNCXd9Dfqe1"
      },
      "source": [
        "# Word2vec con Gensim\n",
        "\n",
        "En este cuaderno de Jupyter vas a utilizar la biblioteca [Gensim](https://radimrehurek.com/gensim/index.html) para experimentar con word2vec. Este cuaderno está enfocado en la intuición de los conceptos y no en los detalles de implementación. Este cuaderno está inspirado en esta [guía](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html).\n",
        "\n",
        "## 1. Instalación y cargar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKIqnDXXfpiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead7e83c-f6e5-4e34-f404-a7cb090da357"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7Q2jB3frOn"
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBaT8djWkaZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacd50c4-7d04-4fd6-a643-6a9aedc25a1d"
      },
      "source": [
        "model = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZm7iTOoawW"
      },
      "source": [
        "## 2. Similitud de palabras\n",
        "\n",
        "En esta sección veremos cómo conseguir la similitud entre dos palabras utilizando un word embedding ya entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOZfaelLoi4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e54879d-484e-4a3a-b749-3ca0312fec3c"
      },
      "source": [
        "model.similarity(\"king\", \"queen\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6510957"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX-Kk9HZofuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e7a1a2-2d96-401d-a93d-bbace440ddb2"
      },
      "source": [
        "model.similarity(\"king\", \"man\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22942673"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypFK-pLrol3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958bc1f7-7195-47b7-e513-9aa6dbedbb48"
      },
      "source": [
        "model.similarity(\"king\", \"potato\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09978465"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWzZySFormq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0eaad3-1cc7-4108-da12-2296492ccdd3"
      },
      "source": [
        "model.similarity(\"king\", \"king\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GijWs_tx83W"
      },
      "source": [
        "Ahora veremos cómo encontrar las palabras con mayor similitud al conjunto de palabras especificado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytELAWBLk2-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1154ee-a6b3-46df-80bf-7b005acc5b1a"
      },
      "source": [
        "model.most_similar([\"king\", \"queen\"], topn=5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('monarch', 0.7042067050933838),\n",
              " ('kings', 0.6780861616134644),\n",
              " ('princess', 0.6731551885604858),\n",
              " ('queens', 0.6679497957229614),\n",
              " ('prince', 0.6435247659683228)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D4ZS7N3ovxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c92781-a05e-4a1a-dc4c-7172508f092e"
      },
      "source": [
        "model.most_similar([\"tomato\", \"carrot\"], topn=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('carrots', 0.7536594867706299),\n",
              " ('tomatoes', 0.7129638195037842),\n",
              " ('celery', 0.7025030851364136),\n",
              " ('broccoli', 0.6796350479125977),\n",
              " ('cherry_tomatoes', 0.662927508354187)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZFlxKjOyBpu"
      },
      "source": [
        "Pero incluso puedes hacer cosas interesantes como ver qué palabra no corresponde a una lista."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CrZdcBpn3pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "950bd435-c151-4dce-ac1d-b2dd604fd5c9"
      },
      "source": [
        "model.doesnt_match([\"summer\", \"fall\", \"spring\", \"air\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'air'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko09hZ3dqMZ1"
      },
      "source": [
        "## Ejercicios\n",
        "\n",
        "1. Usa el modelo word2vec para hacer un ranking de las siguientes 15 palabras según su similitud con las palabras \"man\" y \"woman\". Para cada par, imprime su similitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ1eD3PpT-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878186d6-246d-4bf9-eee8-492e9be8e059"
      },
      "source": [
        "words = [\n",
        "\"wife\",\n",
        "\"husband\",\n",
        "\"child\",\n",
        "\"queen\",\n",
        "\"king\",\n",
        "\"man\",\n",
        "\"woman\",\n",
        "\"birth\",\n",
        "\"doctor\",\n",
        "\"nurse\",\n",
        "\"teacher\",\n",
        "\"professor\",\n",
        "\"engineer\",\n",
        "\"scientist\",\n",
        "\"president\"]\n",
        "\n",
        "def ranked_words(words, model, target_words):\n",
        "  similarities = {}\n",
        "  for word in words:\n",
        "    for target in target_words:\n",
        "      similarity = model.similarity(word,target)\n",
        "      similarities[(word,target)] = similarity\n",
        "      print(f\"Similarity between {word} and {target}: {similarity}\")\n",
        "  ranked_words = sorted(words, key=lambda word: sum(similarities[(word, target)] for target in target_words) / len(target_words), reverse=True)\n",
        "  return ranked_words\n",
        "\n",
        "\n",
        "target_words = [\"man\",\"woman\"]\n",
        "words = ranked_words(words, model, target_words)\n",
        "\n",
        "for word in words:\n",
        "  print(word)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between wife and man: 0.3292091488838196\n",
            "Similarity between wife and woman: 0.4448240101337433\n",
            "Similarity between husband and man: 0.34499746561050415\n",
            "Similarity between husband and woman: 0.4928138256072998\n",
            "Similarity between child and man: 0.3163333833217621\n",
            "Similarity between child and woman: 0.475003719329834\n",
            "Similarity between queen and man: 0.16658201813697815\n",
            "Similarity between queen and woman: 0.31618136167526245\n",
            "Similarity between king and man: 0.22942672669887543\n",
            "Similarity between king and woman: 0.1284797340631485\n",
            "Similarity between man and man: 1.0\n",
            "Similarity between man and woman: 0.7664012312889099\n",
            "Similarity between woman and man: 0.7664012312889099\n",
            "Similarity between woman and woman: 1.0\n",
            "Similarity between birth and man: 0.11078789085149765\n",
            "Similarity between birth and woman: 0.2147129327058792\n",
            "Similarity between doctor and man: 0.3144896328449249\n",
            "Similarity between doctor and woman: 0.37945857644081116\n",
            "Similarity between nurse and man: 0.2547228932380676\n",
            "Similarity between nurse and woman: 0.44135594367980957\n",
            "Similarity between teacher and man: 0.2500012516975403\n",
            "Similarity between teacher and woman: 0.31357845664024353\n",
            "Similarity between professor and man: 0.09415861964225769\n",
            "Similarity between professor and woman: 0.13077852129936218\n",
            "Similarity between engineer and man: 0.15128928422927856\n",
            "Similarity between engineer and woman: 0.09435377269983292\n",
            "Similarity between scientist and man: 0.158249631524086\n",
            "Similarity between scientist and woman: 0.1548689752817154\n",
            "Similarity between president and man: 0.02842460386455059\n",
            "Similarity between president and woman: 0.06267670542001724\n",
            "man\n",
            "woman\n",
            "husband\n",
            "child\n",
            "wife\n",
            "nurse\n",
            "doctor\n",
            "teacher\n",
            "queen\n",
            "king\n",
            "birth\n",
            "scientist\n",
            "engineer\n",
            "professor\n",
            "president\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKamywnxqxJJ"
      },
      "source": [
        "**2. Completa las siguientes analogías por tu cuenta (sin usar el modelo)**\n",
        "\n",
        "a. king is to throne as judge is to _\n",
        "\n",
        "b. giant is to dwarf as genius is to _\n",
        "\n",
        "c. French is to France as Spaniard is to _\n",
        "\n",
        "d. bad is to good as sad is to _\n",
        "\n",
        "e. nurse is to hospital as teacher is to _\n",
        "\n",
        "f. universe is to planet as house is to _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNRxHuZrXAM"
      },
      "source": [
        "**3. Ahora completa las analogías usando un modelo word2vec**\n",
        "\n",
        "Aquí hay un ejemplo de cómo hacerlo. Puedes resolver analogías como \"A es a B como C es a _\" haciendo A + C - B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4kF08h4qhxM"
      },
      "source": [
        "# man is to woman as king is to ___?\n",
        "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiPbbGsori48"
      },
      "source": [
        "# us is to burger as italy is to ___?\n",
        "model.most_similar(positive=[\"Mexico\", \"burger\"], negative=[\"USA\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# king is to throne as judge is to __?\n",
        "model.most_similar(positive = [\"judge\",\"throne\"], negative = [\"king\"], topn=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uil3GR3AB3Nr",
        "outputId": "cf5356ce-23d6-478a-b643-0e940b9ba58b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('appellate_court', 0.5845253467559814)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# giant is to dwarf as genius is to _?\n",
        "model.most_similar(positive=[\"genius\",\"dwarf\"], negative=[\"giant\"], topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuBUm5qjCd7G",
        "outputId": "35764bc6-e584-48ec-c64b-5d6123841085"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('savant', 0.44152510166168213),\n",
              " ('brilliance', 0.4409405589103699),\n",
              " ('genious', 0.43906503915786743),\n",
              " ('prose_stylist', 0.4382379949092865),\n",
              " ('mathematical_genius', 0.43737608194351196)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French is to France as Spaniard is to _?\n",
        "model.most_similar(positive=[\"spaniard\",\"France\"], negative=[\"french\"],topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8aNCvgQDFcp",
        "outputId": "ad4ca164-913a-4437-c876-8e670b799513"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stade_De', 0.5246504545211792),\n",
              " ('Christophe_Legout', 0.5201050639152527),\n",
              " ('Albert_Montañés', 0.5125465989112854),\n",
              " ('Adrien_Mattenet', 0.5125324726104736),\n",
              " ('Paul_Henri_Matthieu', 0.508332371711731)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bad is to good as sad is to _?\n",
        "model.most_similar(positive=[\"sad\",\"good\"], negative=[\"bad\"], topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9k-aHNEDjZC",
        "outputId": "31f1174b-f1d7-4f86-f651-694603a9b800"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wonderful', 0.6414927840232849),\n",
              " ('happy', 0.6154337525367737),\n",
              " ('great', 0.5803679823875427),\n",
              " ('nice', 0.5683972835540771),\n",
              " ('saddening', 0.5588892698287964)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}